{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters, data load and initial conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we add all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "using PythonCall,CondaPkg\n",
    "JEft=pyimport(\"justetf_scraping\")\n",
    "\n",
    "using CairoMakie\n",
    "using Serialization, CSV, DataFrames, Dates\n",
    "using Statistics, CovarianceEstimation, Random\n",
    "using LinearAlgebra \n",
    "using NLopt\n",
    "\n",
    "include(\"src/src.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- `shift`: a period of time in days over which all the returns will be evaluated.\n",
    "- `risk_free_return`: the return over `shift` period of time considered risk free, e.g. bank deposit.\n",
    "- `half_life`: to compute mean return over `shift` period we use weighted means. The data at the day `today-half_life` will have weight two times smaller than the data from `today`.\n",
    "- `correlation_threshold`: at some moment we will filter our tickers so that no tickers with correlation above the given threshold are left. For faster results use lower threshold!\n",
    "- `update_charts_q`: if `true`, the fresh charts from justETF will be downloaded (this will take around 30 minutes). If `false`, the charts will be loaded from the data folder. If there is no charts there, charts will be loaded from justETF.\n",
    "- `data_starting_date`: ETS that don't have data earlier than this will be thrown away.\n",
    "- `available_tickers_file`: location of the file that lists the tickers we have an access to. If `nothing`, all tickers from the justETF website will be considered available. The file should be a table with \"isin\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift=365;\n",
    "risk_free_return=0.037;\n",
    "half_life=4;\n",
    "correlation_threshold=0.9;\n",
    "update_charts_q=true;\n",
    "data_starting_date=Date(2019);\n",
    "available_tickers_file=nothing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two tables with data:\n",
    " - `DO` stands for \"Data Overview\". There, each row corresponds to a particular ticker. Contains some general information.\n",
    " - `DC` stands for \"Data Charts\". There, the first column contains all the dates and the rest of the columns corresponds to values of tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp=JEft.load_overview(strategy=\"epg-longOnly\")\n",
    "tmp.to_csv(\"tmp.csv\")\n",
    "DO=CSV.read(\"tmp.csv\", DataFrame)\n",
    "rm(\"tmp.csv\");\n",
    "\n",
    "if update_charts_q\n",
    "    DC=up_to_date_charts(DO)\n",
    "else\n",
    "    if isfile(\"data/charts.csv\")\n",
    "        DC=CSV.read(\"data/charts.csv\", DataFrame)\n",
    "    else\n",
    "        DC=up_to_date_charts(DO)\n",
    "    end\n",
    "end\n",
    "\n",
    "tickers_consistent_with_DC=DO.isin .|> x-> x in names(DC)\n",
    "if !all(tickers_consistent_with_DC)\n",
    "    println(\"Attention, new tickets in the overview table. The database is outdated. Ignore, or update the database.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the conditioned tables `DO_acc` and `DC_acc`. In these tables we have only accumulating ETFs with the fund size of at least 50 millions euro. Furthermore, we throw away ETFs which don't have data at least up to `data_starting_date` unless we can compensate it using the data from distributing ETF version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions=(DO.dividends.==\"Accumulating\") .& (DO.size .|> x -> ismissing(x) ? false : x>=50 ) .& tickers_consistent_with_DC\n",
    "\n",
    "DO_acc=DO[conditions,:]\n",
    "DC_acc=DC[:,[\"date\",DO_acc.isin...]]\n",
    "\n",
    "\n",
    "tickers_to_remove=Vector{Int}()\n",
    "for i=2:length(names(DC_acc))\n",
    "    tk=names(DC_acc)[i]\n",
    "    tk_vals=DC_acc[!,tk]\n",
    "\n",
    "    tk_first_day_index=findfirst(x->!(ismissing(x)),tk_vals)\n",
    "\n",
    "    if DC_acc[tk_first_day_index,\"date\"]>data_starting_date\n",
    "        tk_dist_name=dist_name(DO[DO.isin.==tk,:name]...)\n",
    "        tk_dist=begin tmp = DO[DO.name.==tk_dist_name,:isin]; length(tmp)>0 ? tmp[1] : nothing end\n",
    "        \n",
    "        if !(isnothing(tk_dist))\n",
    "            tk_dist_vals=DC[!,tk_dist]\n",
    "            tk_dist_first_day_index=findfirst(x->!(ismissing(x)),tk_dist_vals)\n",
    "            if DC[tk_dist_first_day_index,\"date\"] <= data_starting_date\n",
    "                DC_acc[!,tk].=DC[!,tk_dist]\n",
    "                println(\"Data in $tk was replaced with data in $(tk_dist)\")\n",
    "            end\n",
    "            println(\"$tk does not have enough data and its distributing version does not have it either. $tk will be removed from the database.\")\n",
    "            push!(tickers_to_remove,i-1)\n",
    "        else\n",
    "            println(\"$tk does not have enough data and there is no distributing version of it to compensate. $tk will be removed from the database.\")\n",
    "            push!(tickers_to_remove,i-1)\n",
    "        end\n",
    "    else\n",
    "        println(\"$tk has enough data\")\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Removing tickers with not enough data\")\n",
    "DO_acc=DO_acc[Not(tickers_to_remove),:]\n",
    "DC_acc=DC_acc[:,[\"date\",DO_acc.isin...]];\n",
    "\n",
    "\n",
    "if !isnothing(available_tickers_file)\n",
    "    println(\"Removing unavailable tickers\")\n",
    "\n",
    "    available_tickers=CSV.read(available_tickers_file, DataFrame).isin\n",
    "\n",
    "    availability_filter=DO_acc.isin .|> x-> x ∈ available_tickers\n",
    "\n",
    "    DO_acc=DO_acc[availability_filter,:]\n",
    "    DC_acc=DC_acc[:,[\"date\",DO_acc.isin...]];\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a random ticker and plot its behavior to compare with the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk=rand(names(DC_acc)[2:end])\n",
    "println(\"ticker: \", tk)\n",
    "fig=Figure(;size=(1200,300))\n",
    "ax=Axis(fig[1,1], title=tk)\n",
    "lines!(ax,DC_acc[!,:date],DC_acc[!,tk].-100; color=:black)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will reduce the number of tickers further by dividing them into highly correlated groups and choosing only one ticker from each group (the one with the highest Sharpe ratio). For this, we will need compute the expectation values `R` and covariance matrix `RR` of the annual returns. We also compute the correlation matrix `cor`, which is a normalized version of the covariance matrix. We will use `cor` to distinguish the correlated assets and `R`, `RR` to compute the Sharpe ratio. \n",
    "\n",
    "We compute the covariance matrix by using so called shrinkage estimators. I don't know much about these and use the method here more or less blindly. The idea is that if we don't have a lot of data point (comparing with the number of tickers) then the standard method can be unstable. Shrinkage estimators are supposed to make the result a bit more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,RR,cor=get_R_RR_cor(DC_acc, shift, half_life);\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now correlation matrix, let us look at how tickers with correlations above certain threshold behave. The following cell will chose a random ticker, take `nsample` tickers whose correlation with it lies inside the `correlation_interval` and plot their behavior. The red line is the chosen ticker and the gray lines is the correlated tickers. To make the comparison simpler, all the lines are shifted so that the last point is at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_interval=[0.9,1.0]\n",
    "nsample=100\n",
    "\n",
    "i=rand(1:size(DO_acc,1))\n",
    "tk=names(DC_acc)[i+1]\n",
    "\n",
    "println(\"chosen ticker: $tk\")\n",
    "\n",
    "tk_cor=cor[:,i]\n",
    "\n",
    "if count(tk_cor .|> (x-> x>correlation_interval[1] && x<correlation_interval[2]))>1\n",
    "\n",
    "    correlated_tickers=names(DC_acc)[2:end][tk_cor .|> (x-> x>correlation_interval[1] && x<correlation_interval[2])]\n",
    "    correlation_values=tk_cor[tk_cor .|> (x-> x>correlation_interval[1] && x<correlation_interval[2])]\n",
    "\n",
    "    correlation_color=(correlation_values.-findmin(correlation_values)[1])./(findmax(correlation_values)[1]-findmin(correlation_values)[1])\n",
    "    println(\"amount of tickers in the correlation interval: $(length(correlated_tickers))\")\n",
    "\n",
    "    fig=Figure(;size=(1800,450))\n",
    "    ax=Axis(fig[1,1], title=\"price\")\n",
    "\n",
    "    for i in 1:min(nsample,length(correlated_tickers))\n",
    "        tkc=correlated_tickers[i]    \n",
    "        sh=collect(skipmissing(DC_acc[:,tkc]))[end]\n",
    "        lines!(ax,DC_acc[!,\"date\"], DC_acc[!,tkc].-sh,color=(:black, (correlation_color[i])))\n",
    "    end\n",
    "\n",
    "    sh=collect(skipmissing(DC_acc[:,tk]))[end]\n",
    "    lines!(ax,DC_acc[!,\"date\"], DC_acc[!,tk].-sh, color=:red)\n",
    "    \n",
    "    ax=Axis(fig[1,2], title=\"relative return over $shift days\")\n",
    "\n",
    "\n",
    "    for i in 1:min(nsample,length(correlated_tickers))\n",
    "        tkc=correlated_tickers[i]    \n",
    "        lines!(ax, DC_acc[(1+shift):end,\"date\"] ,percent_return(DC_acc[:,tkc],shift),color=(:black, (correlation_color[i])))\n",
    "    end\n",
    "\n",
    "    lines!(ax,DC_acc[(1+shift):end,\"date\"], percent_return(DC_acc[!,tk],shift), color=:red)\n",
    "\n",
    "    color_range = (minimum(correlation_values),maximum(correlation_values))\n",
    "    cmap = :grays\n",
    "\n",
    "    cbar = Colorbar(fig[1,3], colormap=Reverse(cmap), limits=color_range,\n",
    "    flipaxis=true, label=\"Correlation\")\n",
    "\n",
    "\n",
    "    fig \n",
    "else\n",
    "    println(\"Ticker is not correlated with other tickers in the given correlation interval, please try again.\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below runs the correlation reducing procedure resulting in the data collection `DC_acc_cr` such that non of two tickers in `DC_acc_cr` are correlated above `correlation_theshold`. After the procedure is done, we also define the data collection `DO_acc_cr` with general information about tickers from `DC_acc_cr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_acc_cr=copy(DC_acc)\n",
    "R,RR,cor=get_R_RR_cor(DC_acc_cr, shift, half_life);\n",
    "\n",
    "while length(cor[cor.>correlation_threshold])>size(cor,1)\n",
    "    R,RR,cor=get_R_RR_cor(DC_acc_cr, shift, half_life)\n",
    "    println(\"number of tickers: \", size(cor,1))\n",
    "\n",
    "\n",
    "    cnt=1\n",
    "    ct_filter=(cor[:,cnt].>correlation_threshold)\n",
    "    while cnt<=length(R)\n",
    "        ct_filter=(cor[:,cnt].>correlation_threshold)\n",
    "        if count(ct_filter)>1\n",
    "            break\n",
    "        end\n",
    "        cnt+=1\n",
    "    end\n",
    "\n",
    "    if cnt>length(R)\n",
    "        break\n",
    "    end\n",
    "\n",
    "    correlated_tickers_ind=findall(x->x==true, ct_filter)\n",
    "\n",
    "    sharpe_ratios=[]\n",
    "    for tk ∈ correlated_tickers_ind\n",
    "        push!(sharpe_ratios,(R[tk]-risk_free_return)/(sqrt(RR[tk,tk])))\n",
    "    end\n",
    "    best_ticker_ind=correlated_tickers_ind[findmax(sharpe_ratios)[2]]\n",
    "    \n",
    "    filter!(x->x!=best_ticker_ind,correlated_tickers_ind)\n",
    "\n",
    "    tickers_to_remove=names(DC_acc_cr)[2:end][correlated_tickers_ind]\n",
    "\n",
    "    DC_acc_cr=DC_acc_cr[!,Not(tickers_to_remove)]\n",
    "end\n",
    "\n",
    "DO_acc_cr=DO_acc[DO_acc.isin .|> (x->x ∈ names(DC_acc_cr)), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to the optimization, let us define some parameters:\n",
    " - `USD_bound`: gives a constrain on the total amount of the USD based tickers\n",
    " - `entropy_factor`: we add entropy times `entropy_factor` into the objective function to improve diversification.\n",
    " - `lowest_share`: if not zero, the code will run optimization twice. In the second time, only tickers with share (in the previous solution) larger than `lowest_share` will be used.\n",
    " - `xtol_rel_1`: In the first optimization round, if the relative change of the weight vector after a step is less than `xtol_rel_1` the search stops\n",
    " - `xtol_rel_2`: In the second optimization round, if the relative change of the weight vector after a step is less than `xtol_rel_2` the search stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USD_bound=0.5\n",
    "entropy_factor=0.2;\n",
    "lowest_share=0.01;\n",
    "xtol_rel_1=1e-4;\n",
    "xtol_rel_2=1e-6;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us proceed to the optimization. It is still semi-global. ISRES algorithm does not guaranties to find global optimum, but it tries to avoid local traps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,RR,cor=get_R_RR_cor(DC_acc_cr, shift, half_life)\n",
    "USD_filter=DO_acc_cr.currency.==\"USD\"\n",
    "\n",
    "\n",
    "function entropy(w)\n",
    "    -sum((w).*log.(w))\n",
    "end\n",
    "function sharpe_ratio(w) \n",
    "    return (dot(R,w)-risk_free_return)/sqrt(dot(w,RR*w))\n",
    "end\n",
    "function objective(w, grad)\n",
    "    if length(grad) > 0\n",
    "        wCw_root=(dot(w,RR*w))^1/2\n",
    "        grad.=R/wCw_root-(dot(R,w)-risk_free_return)*(RR*w)/(wCw_root)^3-entropy_factor*(log.(w).+1)\n",
    "    end\n",
    "    return sharpe_ratio(w)+entropy_factor*entropy(w)\n",
    "end\n",
    "function sum_constrain(w, grad)\n",
    "    if length(grad) > 0\n",
    "        grad.=1\n",
    "    end\n",
    "    return sum(w)-1\n",
    "end\n",
    "function USD_constrain(w,grad)\n",
    "    if length(grad) > 0\n",
    "        grad.=USD_filter\n",
    "    end\n",
    "    return sum(w[USD_filter])-USD_bound\n",
    "end\n",
    "\n",
    "\n",
    "opt = NLopt.Opt(:GN_ISRES, length(R))\n",
    "NLopt.lower_bounds!(opt, zeros(length(R)))\n",
    "NLopt.upper_bounds!(opt, ones(length(R)))\n",
    "equality_constraint!(opt, sum_constrain, 1e-8)\n",
    "inequality_constraint!(opt, USD_constrain, 1e-8)\n",
    "\n",
    "NLopt.max_objective!(opt, objective)\n",
    "\n",
    "NLopt.xtol_rel!(opt, xtol_rel_1)\n",
    "\n",
    "w_opt=rand(length(R))\n",
    "w_opt/=sum(w_opt)\n",
    "\n",
    "max_f, w_opt, ret = NLopt.optimize!(opt, w_opt)\n",
    "num_evals = NLopt.numevals(opt)\n",
    "\n",
    "ret_risk_r=dot(R,w_opt)/sqrt(dot(w_opt,RR*w_opt))\n",
    "\n",
    "println(\n",
    "    \"\"\"\n",
    "    First round of optimization (ISRES):\n",
    "    solution status           : $ret\n",
    "    sum of weights            : $(sum(w_opt))\n",
    "    USD share                 : $(sum(w_opt[USD_filter]))\n",
    "    # function evaluation     : $num_evals\n",
    "    -------------------------------------------------\n",
    "    objective value           : $max_f\n",
    "    entropy                   : $(entropy(w_opt))\n",
    "    mean return               : $(dot(R,w_opt))\n",
    "    mean risk                 : $(sqrt(dot(w_opt,RR*w_opt)))\n",
    "    return/risk ratio         : $(ret_risk_r)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "low_share_filter=(w_opt.>=lowest_share)\n",
    "if lowest_share!=0\n",
    "    println(\"The optimization problem will be restricted to $(count(low_share_filter)) tickers with share more than $lowest_share\")\n",
    "    R=R[low_share_filter]\n",
    "    RR=RR[low_share_filter,low_share_filter]\n",
    "    w_opt=w_opt[low_share_filter]\n",
    "    w_opt./=sum(w_opt)\n",
    "    USD_filter=USD_filter[low_share_filter]\n",
    "\n",
    "    DO_acc_cr=DO_acc_cr[low_share_filter,:]\n",
    "    DC_acc_cr=DC_acc_cr[:,[\"date\",DO_acc_cr.isin...]];\n",
    "end\n",
    "\n",
    "opt = NLopt.Opt(:GN_ISRES, length(R))\n",
    "\n",
    "NLopt.lower_bounds!(opt, ones(length(R))*lowest_share)\n",
    "NLopt.upper_bounds!(opt, ones(length(R)))\n",
    "equality_constraint!(opt, sum_constrain, 1e-8)\n",
    "inequality_constraint!(opt, USD_constrain, 1e-8)\n",
    "\n",
    "NLopt.max_objective!(opt, objective)\n",
    "\n",
    "\n",
    "NLopt.xtol_rel!(opt, xtol_rel_2)\n",
    "\n",
    "max_f, w_opt, ret = NLopt.optimize!(opt, w_opt)\n",
    "num_evals = NLopt.numevals(opt)\n",
    "\n",
    "ret_risk_r=dot(R,w_opt)/sqrt(dot(w_opt,RR*w_opt))\n",
    "\n",
    "\n",
    "println(\n",
    "    \"\"\"\n",
    "    Second round of optimization (ISRES):\n",
    "    solution status           : $ret\n",
    "    sum of weights            : $(sum(w_opt))\n",
    "    USD share                 : $(sum(w_opt[USD_filter]))\n",
    "    # function evaluation     : $num_evals\n",
    "    -------------------------------------------------\n",
    "    objective value           : $max_f\n",
    "    entropy                   : $(entropy(w_opt))\n",
    "    mean return               : $(dot(R,w_opt))\n",
    "    mean risk                 : $(sqrt(dot(w_opt,RR*w_opt)))\n",
    "    return/risk ratio         : $(ret_risk_r)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=sortperm(w_opt; rev=true)\n",
    "names_ordered=DO_acc_cr.name[p]\n",
    "isin_ordered=DO_acc_cr.isin[p]\n",
    "w_opt_ord=w_opt[p]\n",
    "for i=1:length(w_opt_ord)\n",
    "    println(names_ordered[i],\" : \", isin_ordered[i], \" | \", round(w_opt_ord[i]; digits=3))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the historical performance of this portfolio (with weights rounded to 3 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt_round=round.(w_opt; digits=3);\n",
    "share_color=(w_opt_round.-findmin(w_opt_round)[1])./(findmax(w_opt_round)[1]-findmin(w_opt_round)[1])\n",
    "\n",
    "tickers_in_portfolio=DO_acc_cr.isin\n",
    "DC_acc_cr_nrm=dropmissing(DC_acc_cr[:, [\"date\",tickers_in_portfolio...]])\n",
    "mapcols!(x->x.-x[1].+100,DC_acc_cr_nrm; cols=Not(\"date\"))\n",
    "\n",
    "portfolio_value=sum(Matrix(reshape(w_opt_round,1,length(w_opt_round)).*DC_acc_cr_nrm[!,Not(\"date\")]); dims=2)[:]\n",
    "\n",
    "fig=Figure(;size=(1200,300))\n",
    "ax=Axis(fig[1,1], title=\"Portfolio\")\n",
    "\n",
    "# Plot constituents first\n",
    "constituent_line = nothing\n",
    "for tk in names(DC_acc_cr_nrm)[2:end][w_opt_round.>1e-10]\n",
    "    tk_ind=findfirst(x->x==tk,names(DC_acc_cr_nrm)[2:end])\n",
    "    constituent_line = lines!(ax,DC_acc_cr_nrm[!,\"date\"], DC_acc_cr_nrm[!,tk].-100; color=(:black,share_color[tk_ind]))\n",
    "end\n",
    "\n",
    "# Plot portfolio\n",
    "portfolio_line = lines!(ax,DC_acc_cr_nrm[!,\"date\"], portfolio_value.-100; color=:red)\n",
    "\n",
    "# Add legend with explicit colors\n",
    "axislegend(ax, [LineElement(linewidth=2, color=:red), LineElement(linewidth=2, color=:black)], [\"portfolio\", \"constituents\"], position=:lt)\n",
    "\n",
    "# Add colorbar for shares\n",
    "cbar = Colorbar(fig[1, 2], limits=(minimum(w_opt_round), maximum(w_opt_round)),\n",
    "                colormap=Reverse(:grays), label=\"Share\")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us safe the result of the optimization together with the data used for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(\"portfolio\", Dict(\"overview_table\"=>DO_acc_cr, \"optimal_portfolio\"=>w_opt, \"USD_bound\"=>USD_bound, \"entropy_factor\"=>entropy_factor, \"lowest_share\"=>lowest_share))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement ideas\n",
    "\n",
    "## performance\n",
    "\n",
    "1. Understand better the covariance matrix computation as now we are using it blindly.\n",
    "2. For now we are using semi-global optimization technique. It does have a way to avoid local optima, however, there is no guarantee that it will not stuck there for sure. Can we do better?    \n",
    "3. It would be nice to compare the perfromance of the optimized portfolio with its perturbations to understand how much of the deviation is tolerable and when one must do rebalancing.\n",
    "   \n",
    "## cosmetics\n",
    "\n",
    "1. It would be nice to see the first and the last dates on the historical plot. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
